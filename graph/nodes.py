import logging
from langchain.schema import HumanMessage, AIMessage
from langchain.prompts import ChatPromptTemplate

logger = logging.getLogger(__name__)

def profile_node(state):
    """Ú¯Ø±Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø±"""
    # Ø§Ú¯Ø± Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø± Ù†Ø§Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ø®Ø·Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
    if not state["user_profile"]:
        state["response"] = "Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯."
        return state
    
    # Ø§Ú¯Ø± Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù†Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ø®Ø·Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
    if not state["user_profile"].get("complete", False):
        state["response"] = "Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø´Ù…Ø§ Ù†Ø§Ù‚Øµ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø¢Ù† Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯."
        return state
    
    # Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø§Ø³ØªØŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
    return state

def router_node(state):
    """Ú¯Ø±Ù‡ ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ùˆ Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ"""
    # Ù†ÙˆØ¹ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø² Ù‚Ø¨Ù„ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ Ø§Ø³Øª (Ø¯Ø± process_with_langgraph)
    return state

def profile_collection_node(llm):
    """Ú¯Ø±Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø¨Ù‡ ØµÙˆØ±Øª Ú¯ÙØªÚ¯ÙˆÛŒÛŒ"""
    def process_profile_conversation(state):
        current_profile = state.get("user_profile", {})
        message = state["messages"][-1].content if state["messages"] else ""
        
        # Create a template for the LLM
        template = """
        Ø´Ù…Ø§ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù† Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯.
        
        Ù¾Ø±ÙˆÙØ§ÛŒÙ„ ÙØ¹Ù„ÛŒ Ú©Ø§Ø±Ø¨Ø±:
        {current_profile}
        
        Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±: {message}
        
        Ù„Ø·ÙØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø§Ø² Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯ Ùˆ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ú©Ù‡ Ú†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù‡Ù†ÙˆØ² Ù†ÛŒØ§Ø² Ø§Ø³Øª.
        
        Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¶Ø±ÙˆØ±ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²:
        - Ù†Ø§Ù… (name)
        - Ù¾Ø§ÛŒÙ‡ ØªØ­ØµÛŒÙ„ÛŒ (grade)
        - ØªØ§Ø±ÛŒØ® Ú©Ù†Ú©ÙˆØ± (exam_date)
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡ (favorite_subjects)
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ù†ÙØ±Øª (disliked_subjects)
        - Ø±Ø´ØªÙ‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± (desired_major)
        
        Ø®Ø±ÙˆØ¬ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø´Ú©Ù„ JSON Ø¨Ø§Ø´Ø¯:
        {
            "extracted_info": {{Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª key-value}},
            "profile_complete": true/false,
            "next_question": "Ø³ÙˆØ§Ù„ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª"
        }
        """
        
        # Fill in the prompt with user's profile and message
        prompt = ChatPromptTemplate.from_template(template)
        current_profile_text = "\n".join([f"- {key}: {value}" for key, value in current_profile.items()])
        
        prompt_values = {
            "current_profile": current_profile_text,
            "message": message,
        }
        
        # Get response from LLM
        messages = prompt.format_messages(**prompt_values)
        response = llm.invoke(messages)
        
        # Process the response
        import json
        try:
            # Extract the JSON part
            response_text = response.content
            # Find JSON object in the response
            import re
            json_match = re.search(r'({.*})', response_text, re.DOTALL)
            if json_match:
                response_json = json.loads(json_match.group(1))
                state["response"] = response_json
            else:
                state["response"] = {
                    "extracted_info": {},
                    "profile_complete": False,
                    "next_question": "Ù…ØªÙˆØ¬Ù‡ Ù†Ø´Ø¯Ù…. Ù„Ø·ÙØ§Ù‹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø®ÙˆØ¯ Ø¨Ø¯Ù‡ÛŒØ¯ (Ù†Ø§Ù…ØŒ Ù¾Ø§ÛŒÙ‡ ØªØ­ØµÛŒÙ„ÛŒØŒ ØªØ§Ø±ÛŒØ® Ú©Ù†Ú©ÙˆØ± Ùˆ...)."
                }
        except json.JSONDecodeError:
            state["response"] = {
                "extracted_info": {},
                "profile_complete": False,
                "next_question": "Ù…ØªÙˆØ¬Ù‡ Ù†Ø´Ø¯Ù…. Ù„Ø·ÙØ§Ù‹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø®ÙˆØ¯ Ø¨Ø¯Ù‡ÛŒØ¯ (Ù†Ø§Ù…ØŒ Ù¾Ø§ÛŒÙ‡ ØªØ­ØµÛŒÙ„ÛŒØŒ ØªØ§Ø±ÛŒØ® Ú©Ù†Ú©ÙˆØ± Ùˆ...)."
            }
        
        return state
    
    return process_profile_conversation

def study_plan_node(llm):
    """Ú¯Ø±Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…Ø·Ø§Ù„Ø¹Ø§ØªÛŒ"""
    def generate_study_plan(state):
        user_profile = state["user_profile"]
        message = state["messages"][-1].content if state["messages"] else ""
        
        # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø±Ø§ÛŒ LLM
        template = """
        Ø´Ù…Ø§ Ù…Ø´Ø§ÙˆØ± ØªØ­ØµÛŒÙ„ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ² Ù‡Ø³ØªÛŒØ¯ Ùˆ Ø¨Ø§ÛŒØ¯ ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…Ø·Ø§Ù„Ø¹Ø§ØªÛŒ Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯.
        
        Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²:
        - Ù†Ø§Ù…: {name}
        - Ù¾Ø§ÛŒÙ‡ ØªØ­ØµÛŒÙ„ÛŒ: {grade}
        - ØªØ§Ø±ÛŒØ® Ú©Ù†Ú©ÙˆØ±: {exam_date}
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡: {favorite_subjects}
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ù†ÙØ±Øª: {disliked_subjects}
        - Ø±Ø´ØªÙ‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±: {desired_major}
        
        Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²: {message}
        
        Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…Ø·Ø§Ù„Ø¹Ø§ØªÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙÙˆÙ‚ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
        Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ùˆ Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…Ø´Ø§ÙˆØ±Ø§Ù†Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.
        Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:
        1. Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
        2. Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø±ÙˆØ³
        3. ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ù†ÙØ±Øª
        4. Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù…Ø¤Ø«Ø±
        
        Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ù¾Ø§Ø³Ø® Ø¬Ø°Ø§Ø¨â€ŒØªØ± Ø´ÙˆØ¯. ğŸ“šâœï¸â°ğŸ“
        """
        
        # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø±
        prompt = ChatPromptTemplate.from_template(template)
        
        prompt_values = {
            "name": user_profile.get("name", "Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²"),
            "grade": user_profile.get("grade", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "exam_date": user_profile.get("exam_date", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "favorite_subjects": ", ".join(user_profile.get("favorite_subjects", [])),
            "disliked_subjects": ", ".join(user_profile.get("disliked_subjects", [])),
            "desired_major": user_profile.get("desired_major", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "message": message
        }
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² LLM
        messages = prompt.format_messages(**prompt_values)
        response = llm.invoke(messages)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ø± state
        state["response"] = response.content
        return state
    
    return generate_study_plan

def performance_analysis_node(llm):
    """Ú¯Ø±Ù‡ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ø²Ù…ÙˆÙ†"""
    def analyze_performance(state):
        user_profile = state["user_profile"]
        exam_results = state.get("exam_results", {})
        
        # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø±Ø§ÛŒ LLM
        template = """
        Ø´Ù…Ø§ Ù…Ø´Ø§ÙˆØ± ØªØ­ØµÛŒÙ„ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ² Ù‡Ø³ØªÛŒØ¯ Ùˆ Ø¨Ø§ÛŒØ¯ Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…ÙˆÙ† Ø§Ùˆ Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.
        
        Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²:
        - Ù†Ø§Ù…: {name}
        - Ù¾Ø§ÛŒÙ‡ ØªØ­ØµÛŒÙ„ÛŒ: {grade}
        - ØªØ§Ø±ÛŒØ® Ú©Ù†Ú©ÙˆØ±: {exam_date}
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡: {favorite_subjects}
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ù†ÙØ±Øª: {disliked_subjects}
        - Ø±Ø´ØªÙ‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±: {desired_major}
        
        Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…ÙˆÙ†:
        {exam_results}
        
        Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…ÙˆÙ† Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙÙˆÙ‚ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
        Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ùˆ Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…Ø´Ø§ÙˆØ±Ø§Ù†Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.
        ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:
        1. Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù
        2. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø¯Ø±ÙˆØ³ Ù…Ø®ØªÙ„Ù
        3. ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÙˆØ³ Ø¶Ø¹ÛŒÙâ€ŒØªØ±
        4. Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØª
        
        Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ù¾Ø§Ø³Ø® Ø¬Ø°Ø§Ø¨â€ŒØªØ± Ø´ÙˆØ¯. ğŸ“ŠğŸ“ˆğŸ“‰ğŸ“š
        """
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…ÙˆÙ†
        exam_results_text = "\n".join([f"- {subject}: {score}" for subject, score in exam_results.items()])
        
        # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø± Ùˆ Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…ÙˆÙ†
        prompt = ChatPromptTemplate.from_template(template)
        
        prompt_values = {
            "name": user_profile.get("name", "Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²"),
            "grade": user_profile.get("grade", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "exam_date": user_profile.get("exam_date", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "favorite_subjects": ", ".join(user_profile.get("favorite_subjects", [])),
            "disliked_subjects": ", ".join(user_profile.get("disliked_subjects", [])),
            "desired_major": user_profile.get("desired_major", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "exam_results": exam_results_text
        }
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² LLM
        messages = prompt.format_messages(**prompt_values)
        response = llm.invoke(messages)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ø± state
        state["response"] = response.content
        return state
    
    return analyze_performance

def general_chat_node(llm):
    """Ú¯Ø±Ù‡ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ"""
    def generate_general_response(state):
        user_profile = state["user_profile"]
        message = state["messages"][-1].content if state["messages"] else ""
        memory = state["memory"]
        
        # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø±Ø§ÛŒ LLM
        template = """
        Ø´Ù…Ø§ Ù…Ø´Ø§ÙˆØ± ØªØ­ØµÛŒÙ„ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ² Ù‡Ø³ØªÛŒØ¯ Ùˆ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ ÛŒØ§ Ù¾ÛŒØ§Ù… Ø§Ùˆ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.
        
        Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²:
        - Ù†Ø§Ù…: {name}
        - Ù¾Ø§ÛŒÙ‡ ØªØ­ØµÛŒÙ„ÛŒ: {grade}
        - ØªØ§Ø±ÛŒØ® Ú©Ù†Ú©ÙˆØ±: {exam_date}
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡: {favorite_subjects}
        - Ø¯Ø±ÙˆØ³ Ù…ÙˆØ±Ø¯ Ù†ÙØ±Øª: {disliked_subjects}
        - Ø±Ø´ØªÙ‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±: {desired_major}
        
        ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡:
        {chat_history}
        
        Ù¾ÛŒØ§Ù… Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²: {message}
        
        Ù„Ø·ÙØ§Ù‹ Ù¾Ø§Ø³Ø®ÛŒ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ØŒ Ù…ÙÛŒØ¯ Ùˆ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù¾ÛŒØ§Ù… Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ² Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
        Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ùˆ Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…Ø´Ø§ÙˆØ±Ø§Ù†Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.
        Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ù¾Ø§Ø³Ø® Ø¬Ø°Ø§Ø¨â€ŒØªØ± Ø´ÙˆØ¯.
        """
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª
        chat_history = "\n".join([f"- {msg}" for msg in memory])
        
        # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø± Ùˆ Ù¾ÛŒØ§Ù…
        prompt = ChatPromptTemplate.from_template(template)
        
        prompt_values = {
            "name": user_profile.get("name", "Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²"),
            "grade": user_profile.get("grade", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "exam_date": user_profile.get("exam_date", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "favorite_subjects": ", ".join(user_profile.get("favorite_subjects", [])),
            "disliked_subjects": ", ".join(user_profile.get("disliked_subjects", [])),
            "desired_major": user_profile.get("desired_major", "Ù†Ø§Ù…Ø´Ø®Øµ"),
            "chat_history": chat_history,
            "message": message
        }
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² LLM
        messages = prompt.format_messages(**prompt_values)
        response = llm.invoke(messages)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ø± state
        state["response"] = response.content
        return state
    
    return generate_general_response